# -*- coding: utf-8 -*-
"""Copy of Propuesta1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L1_VN-2s8mCAb67MtAJ9NL2d_UCs-kY9

### Cargar los datos

El primer paso es cargar los datos de fotometría de los archivos CSV que contienen las observaciones de curvas de luz de diferentes tipos de objetos astronómicos. Se combinan las observaciones de Cepheid, RRL y EB en un único dataframe y se agregan las etiquetas correspondientes para la clasificación:

- 0 para Cepheid.
- 1 para RRL.
- 2 para EB.
"""

# Función para cargar los datos (Cepheid, RRL, y EB)
def load_data(cepheid_path, rrl_path, eb_path):
    # Cargar los archivos CSV
    cepheid_phot = pd.read_csv(cepheid_path)
    rrl_phot = pd.read_csv(rrl_path)
    eb_phot = pd.read_csv(eb_path)  # Cargar el archivo de EB

    # Crear etiquetas: 0 para Cepheid, 1 para RRL, 2 para EB
    cepheid_phot['label'] = 0
    rrl_phot['label'] = 1
    eb_phot['label'] = 2

    # Combinar los tres datasets
    combined_data = pd.concat([cepheid_phot, rrl_phot, eb_phot], ignore_index=True)
    return combined_data

"""### Filtrar la banda fotométrica i

El siguiente paso del pipeline es filtrar las observaciones para trabajar únicamente con las que se realizaron en la banda i, que representa una parte específica del espectro electromagnético (cerca del infrarrojo). Cada observación tiene un valor de flujo (FLUXCAL) en una banda fotométrica (BAND).
"""

# Clase para filtrar por banda
class FilterBand(TransformerMixin, BaseEstimator):
    def __init__(self, band='i'):
        self.band = band

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return X[X['BAND'] == self.band]

"""### Normalización de los datos (FLUXCAL)

Las redes neuronales funcionan mejor cuando todas las variables de entrada tienen magnitudes similares. Si los valores de flujo (FLUXCAL) tiene diferentes escalas, la red neuronal podría tener dificultades para aprender los patrones de forma eficiente.

**FLUXCAL**: El flujo calibrado puede tener variaciones significativas entre observaciones, por lo que su normalización asegura que el modelo trate a todas las variables de manera equitativa.
"""

# Clase para normalizar solo FLUXCAL
class NormalizeData(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.scaler = StandardScaler()

    def fit(self, X, y=None):
        # Ajustar el escalador solo en FLUXCAL
        self.scaler.fit(X[['FLUXCAL']])
        return self

    def transform(self, X):
        # Transformar solo FLUXCAL usando .loc[] para evitar el SettingWithCopyWarning
        X.loc[:, 'FLUXCAL'] = self.scaler.transform(X[['FLUXCAL']])
        return X

"""X.loc[:, 'FLUXCAL']: Asegura que se esta modificando correctamente la columna FLUXCAL en el DataFrame original, sin crear una vista temporal.

### Creación de secuencias temporales

Aquí creamos secuencias de observaciones de longitud fija a partir de los datos normalizados.
"""

# Clase para crear secuencias temporales
class CreateSequences(BaseEstimator, TransformerMixin):
    def __init__(self, seq_length=10):
        self.seq_length = seq_length

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        sequences = []
        labels = []
        for i in range(len(X) - self.seq_length):
            seq = X.iloc[i:i+self.seq_length][['MJD', 'FLUXCAL']].values
            label = X.iloc[i+self.seq_length-1]['label']
            sequences.append(seq)
            labels.append(label)
        return np.array(sequences), np.array(labels)

"""### Lomb-Scargle Periodogram

El Lomb-Scargle Periodogram es una técnica usada para detectar periocidad en datos que son irregularmente muestreados (es decir, cuando los datos no están igualmente espaciados en el tiempo, como en muchas observaciones astronómicas).
"""

class AddBestPeriod(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        # Verificar si X es una tupla, en cuyo caso descomponerla
        if isinstance(X, tuple):
            X, y = X  # Descomponer en características (X) y etiquetas (y)

        best_periods = []

        # Iterar sobre las secuencias de entrenamiento para calcular los mejores períodos
        for i in range(len(X)):
            time = X[i, :, 0]  # MJD (tiempo)
            flux = X[i, :, 1]  # FLUXCAL (flujo)

            # Calcular el mejor período usando Lomb-Scargle (en lugar de LombScargleFast)
            best_period = self.calculate_best_period(time, flux)
            best_periods.append(best_period)

        # Convertir la lista de mejores períodos a un array numpy y agregarlo a las secuencias
        best_periods = np.array(best_periods).reshape(-1, 1)
        X_with_periods = np.concatenate([X, best_periods], axis=1)
        return X_with_periods

    def calculate_best_period(self, time, flux):
        # Instancia el modelo Lomb-Scargle en lugar de LombScargleFast
        model = LombScargle(fit_period=True)

        # Configura el rango de búsqueda del período
        model.optimizer.period_range = (0.2, 1.2)

        # Ajusta el modelo a los datos de tiempo y flujo
        model.fit(time, flux)

        # Retorna el mejor período encontrado
        return model.best_period

"""### Pipeline para el procesamiento completo"""

def create_pipeline():
    pipeline = Pipeline([
        ('filter_band', FilterBand(band='i')),          # Filtrar la banda i
        ('normalize', NormalizeData()),                 # Normalizar solo FLUXCAL
        ('create_sequences', CreateSequences(seq_length=10)),  # Crear secuencias temporales
        ('add_best_period', AddBestPeriod())            # Calcular y agregar el mejor período
    ])
    return pipeline

def run_pipeline(cepheid_path, rrl_path, eb_path):
    # Cargar los datos
    combined_data = load_data(cepheid_path, rrl_path, eb_path)
    print("datos combinados")

    # Crear el pipeline
    pipeline = create_pipeline()
    print("pipeline creado")

    # Ejecutar el pipeline
    print("pipeline ejecutado")
    X, y = pipeline.fit_transform(combined_data)
    print("pipeline finalizado")

    # Dividir los datos en conjunto de entrenamiento y validación
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # Mostrar los tamaños de los conjuntos de datos
    print(f'Tamaño del conjunto de entrenamiento: {X_train.shape}')
    print(f'Tamaño del conjunto de validación: {X_val.shape}')

    return X_train, X_val, y_train, y_val

# Rutas a los archivos
cepheid_path = '/datasets/Cepheid_PHOT.csv'
rrl_path = '/datasets/RRL_PHOT.csv'
eb_path = '/datasets/EB_PHOT.csv'

"""### Ejecutar pipeline"""

X_train, X_val, y_train, y_val = run_pipeline(cepheid_path, rrl_path, eb_path)


"""## **5.1 Contrucción de pipeline**"""



import cudf
import cupy as cp
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from joblib import Parallel, delayed
from gatspy.periodic import LombScargle
import numpy as np

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam

def build_lstm_model(input_shape, num_classes):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=input_shape))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Definir la forma de entrada y el número de clases
input_shape = (X_train.shape[1], X_train.shape[2])  # Longitud de secuencia, número de características
num_classes = 3  # Cepheid, RRL, EB

# Crear el modelo y entrenarlo en GPU
with tf.device('/device:GPU:0'):  # Asegura que se ejecute en la GPU si está disponible
    model = build_lstm_model(input_shape, num_classes)

# Entrenar el modelo en la GPU
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)

# Evaluar el modelo en el conjunto de validación
loss, accuracy = model.evaluate(X_val, y_val)
print(f'Pérdida en validación: {loss}')
print(f'Precisión en validación: {accuracy}')

"""## Gráfica de la función de pérdida"""

plt.plot(history.history['loss'], label='Pérdida de entrenamiento')
plt.plot(history.history['val_loss'], label='Pérdida de validación')
plt.title('Evolución de la pérdida durante el entrenamiento')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.show()

"""## Predicciones"""

# Realizar predicciones en el conjunto de validación
predicciones = model.predict(X_val)
predicciones_clases = np.argmax(predicciones, axis=1)

# Mostrar las primeras 10 predicciones
print(predicciones_clases[:10])

# Mostrar etiquetas reales
print(y_val[:10])