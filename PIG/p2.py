# -*- coding: utf-8 -*-
"""P2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D8peObc9gmQMk4SQmGr_UV-5YTGzRX_i
"""

import cudf
import cupy as cp
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from gatspy.periodic import LombScargle
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Function to load and combine datasets
def load_data(cepheid_path, rrl_path, eb_path):
    # Load datasets
    cepheid_phot = pd.read_csv(cepheid_path)
    rrl_phot = pd.read_csv(rrl_path)
    eb_phot = pd.read_csv(eb_path)

    # Assign labels
    cepheid_phot['label'] = 0
    rrl_phot['label'] = 1
    eb_phot['label'] = 2

    # Combine datasets
    combined_data = pd.concat([cepheid_phot, rrl_phot, eb_phot], ignore_index=True)
    return combined_data

# Transformer to filter photometric bands
class FilterBands(BaseEstimator, TransformerMixin):
    def __init__(self, bands=('i', 'r')):
        self.bands = bands

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return X[X['BAND'].isin(self.bands)]

# Transformer to normalize FLUXCAL
class NormalizeData(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.scaler = StandardScaler()

    def fit(self, X, y=None):
        self.scaler.fit(X[['FLUXCAL']])
        return self

    def transform(self, X, y=None):
        X['FLUXCAL'] = self.scaler.transform(X[['FLUXCAL']])
        return X

# Transformer to create time sequences
class CreateSequences(BaseEstimator, TransformerMixin):
    def __init__(self, seq_length=10):
        self.seq_length = seq_length

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        sequences, labels = [], []
        for label in X['label'].unique():
            group = X[X['label'] == label]
            for i in range(len(group) - self.seq_length):
                seq = group.iloc[i:i+self.seq_length][['MJD', 'FLUXCAL']].values
                sequences.append(seq)
                labels.append(label)
        return np.array(sequences), np.array(labels)

# Transformer to calculate Lomb-Scargle periods
class AddBestPeriod(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        sequences, labels = X
        best_periods = []
        for seq in sequences:
            time, flux = seq[:, 0], seq[:, 1]
            model = LombScargle().fit(time, flux)
            best_periods.append(model.best_period)
        best_periods = np.array(best_periods).reshape(-1, 1)
        sequences = np.concatenate([sequences, best_periods], axis=2)
        return sequences, labels

# Function to build LSTM model
def build_lstm_model(input_shape, num_classes):
    model = Sequential([
        LSTM(64, activation='relu', input_shape=input_shape, return_sequences=True),
        Dropout(0.2),
        LSTM(32, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Function to create the pipeline
def create_pipeline():
    return Pipeline([
        ('filter_bands', FilterBands(bands=('i', 'r'))),
        ('normalize', NormalizeData()),
        ('create_sequences', CreateSequences(seq_length=10)),
        ('add_best_period', AddBestPeriod())
    ])

# Main execution function
def run_pipeline(cepheid_path, rrl_path, eb_path):
    combined_data = load_data(cepheid_path, rrl_path, eb_path)
    pipeline = create_pipeline()
    X, y = pipeline.fit_transform(combined_data)

    # Train-test split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train LSTM model
    input_shape = (X_train.shape[1], X_train.shape[2])
    model = build_lstm_model(input_shape, num_classes=3)

    with tf.device('/device:GPU:0'):
        history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_val, y_val))

    # Evaluate the model
    loss, accuracy = model.evaluate(X_val, y_val)
    print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')

    return history

# Example file paths
cepheid_path = '/workspace/datasets/Cepheid_PHOT.csv'
rrl_path = '/workspace/datasets/RRL_PHOT.csv'
eb_path = '/workspace/datasets/EB_PHOT.csv'

# Run the pipeline
history = run_pipeline(cepheid_path, rrl_path, eb_path)