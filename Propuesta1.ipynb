{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7z4ukcRiunJj6QQLRyHI5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiapapadron/The-DESC-ELAsTiCC-Challenge/blob/main/Propuesta1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Importación de Librerías**"
      ],
      "metadata": {
        "id": "MAO0-B_d1BMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_yJncR2Cz0qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Carga de archivos**"
      ],
      "metadata": {
        "id": "XdAsf7KJ1I4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subir archivos CSV\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Cargar los archivos CSV en DataFrames de pandas\n",
        "df_cepheid_head = pd.read_csv('Cepheid_HEAD.csv')\n",
        "df_cepheid_phot = pd.read_csv('Cepheid_PHOT.csv')\n",
        "df_rrl_head = pd.read_csv('RRL_HEAD.csv')\n",
        "df_rrl_phot = pd.read_csv('RRL_PHOT.csv')\n",
        "df_eb_head = pd.read_csv('EB_HEAD.csv')\n",
        "df_eb_phot = pd.read_csv('EB_PHOT.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "zLN58881x_-x",
        "outputId": "10b786d8-db1e-47ee-dc0a-5037aa390fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a6d082b-f5c0-4b1c-afb5-78c9ae38e287\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a6d082b-f5c0-4b1c-afb5-78c9ae38e287\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Verificación de carga exitosa**"
      ],
      "metadata": {
        "id": "esugYO-B1M_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeras filas para verificar la carga\n",
        "print(\"Primeras filas de Cepheid_HEAD:\")\n",
        "df_cepheid_head.head()"
      ],
      "metadata": {
        "id": "JIRWOsa-yWWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrimeras filas de Cepheid_PHOT:\")\n",
        "df_cepheid_phot.head()"
      ],
      "metadata": {
        "id": "jF5oBZ-7zk7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrimeras filas de RRL_HEAD:\")\n",
        "df_rrl_head.head()"
      ],
      "metadata": {
        "id": "woEAH3TQzmeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrimeras filas de RRL_PHOT:\")\n",
        "df_rrl_phot.head()"
      ],
      "metadata": {
        "id": "4c6xycSrzn24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrimeras filas de EB_HEAD:\")\n",
        "df_eb_head.head()"
      ],
      "metadata": {
        "id": "JR59BRXhRXMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPrimeras filas de EB_PHOT:\")\n",
        "df_eb_phot.head()"
      ],
      "metadata": {
        "id": "-OOD_p8tRhRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionemos por ejemplo los datos del primer objeto en los archivos Cepheid\n",
        "i = 0\n",
        "\n",
        "# Usamos los índices de PTROBS_MIN y PTROBS_MAX para obtener las filas correctas\n",
        "this_df_cepheid = df_cepheid_phot[df_cepheid_head[\"PTROBS_MIN\"][i]-1 : df_cepheid_head[\"PTROBS_MAX\"][i]-1]\n",
        "bands = set(this_df_cepheid[\"BAND\"])\n",
        "\n",
        "# Elegimos los colores para cada filtro de banda\n",
        "colors = {\n",
        "    \"u\": \"blue\",\n",
        "    \"g\": \"green\",\n",
        "    \"r\": \"orange\",\n",
        "    \"i\": \"red\",\n",
        "    \"z\": \"purple\",\n",
        "    \"Y\": \"yellow\",\n",
        "}\n",
        "\n",
        "# Nos aseguramos de que los nombres de las bandas estén bien formateados\n",
        "for k, v in frozenset(colors.items()):\n",
        "    colors[k + \" \"] = v\n",
        "\n",
        "# Graficar los datos\n",
        "for filt in bands:\n",
        "    tb = this_df_cepheid[this_df_cepheid[\"BAND\"] == filt]\n",
        "    c = colors[filt]\n",
        "    plt.errorbar(\n",
        "        tb[\"MJD\"], tb[\"FLUXCAL\"], tb[\"FLUXCALERR\"], marker=\"o\", ls=\"none\", color=c, label=filt\n",
        "    )\n",
        "    plt.xlabel(\"MJD\")\n",
        "    plt.ylabel(\"Flux\")\n",
        "\n",
        "# Mostrar el título basado en el objeto SNID\n",
        "plt.title(df_cepheid_head[\"SNID\"][i])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xDerHOx_0ls7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos los índices de PTROBS_MIN y PTROBS_MAX para obtener las filas correctas\n",
        "this_df_rrl = df_rrl_phot[df_rrl_head[\"PTROBS_MIN\"][i]-1 : df_rrl_head[\"PTROBS_MAX\"][i]-1]\n",
        "bands = set(this_df_rrl[\"BAND\"])\n",
        "\n",
        "# Elegimos los colores para cada filtro de banda\n",
        "colors = {\n",
        "    \"u\": \"blue\",\n",
        "    \"g\": \"green\",\n",
        "    \"r\": \"orange\",\n",
        "    \"i\": \"red\",\n",
        "    \"z\": \"purple\",\n",
        "    \"Y\": \"yellow\",\n",
        "}\n",
        "\n",
        "# Nos aseguramos de que los nombres de las bandas estén bien formateados\n",
        "for k, v in frozenset(colors.items()):\n",
        "    colors[k + \" \"] = v\n",
        "\n",
        "# Graficar los datos\n",
        "for filt in bands:\n",
        "    tb = this_df_rrl[this_df_rrl[\"BAND\"] == filt]\n",
        "    c = colors[filt]\n",
        "    plt.errorbar(\n",
        "        tb[\"MJD\"], tb[\"FLUXCAL\"], tb[\"FLUXCALERR\"], marker=\"o\", ls=\"none\", color=c, label=filt\n",
        "    )\n",
        "    plt.xlabel(\"MJD\")\n",
        "    plt.ylabel(\"Flux\")\n",
        "\n",
        "# Mostrar el título basado en el objeto SNID\n",
        "plt.title(df_rrl_head[\"SNID\"][i])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mpG6baIo05WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos los índices de PTROBS_MIN y PTROBS_MAX para obtener las filas correctas\n",
        "this_df_eb = df_eb_phot[df_eb_head[\"PTROBS_MIN\"][i]-1 : df_eb_head[\"PTROBS_MAX\"][i]-1]\n",
        "bands = set(this_df_eb[\"BAND\"])\n",
        "\n",
        "# Elegimos los colores para cada filtro de banda\n",
        "colors = {\n",
        "    \"u\": \"blue\",\n",
        "    \"g\": \"green\",\n",
        "    \"r\": \"orange\",\n",
        "    \"i\": \"red\",\n",
        "    \"z\": \"purple\",\n",
        "    \"Y\": \"yellow\",\n",
        "}\n",
        "\n",
        "# Nos aseguramos de que los nombres de las bandas estén bien formateados\n",
        "for k, v in frozenset(colors.items()):\n",
        "    colors[k + \" \"] = v\n",
        "\n",
        "# Graficar los datos\n",
        "for filt in bands:\n",
        "    tb = this_df_eb[this_df_eb[\"BAND\"] == filt]\n",
        "    c = colors[filt]\n",
        "    plt.errorbar(\n",
        "        tb[\"MJD\"], tb[\"FLUXCAL\"], tb[\"FLUXCALERR\"], marker=\"o\", ls=\"none\", color=c, label=filt\n",
        "    )\n",
        "    plt.xlabel(\"MJD\")\n",
        "    plt.ylabel(\"Flux\")\n",
        "\n",
        "# Mostrar el título basado en el objeto SNID\n",
        "plt.title(df_eb_head[\"SNID\"][i])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gc8VNvXWRmbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Primera propuesta de codigo**"
      ],
      "metadata": {
        "id": "oW4L1eAYJHT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cudf-cu12 --extra-index-url=https://pypi.nvidia.com"
      ],
      "metadata": {
        "id": "wifo17Om42uq",
        "outputId": "1cc9ab44-62b0-4ccd-dbd1-21f80b058ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: cudf-cu12 in /usr/local/lib/python3.10/dist-packages (24.10.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (5.5.0)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (12.2.1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (12.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (2024.10.0)\n",
            "Requirement already satisfied: libcudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.10.1)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.60.0)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (1.26.4)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.1)\n",
            "Requirement already satisfied: pandas<2.2.3dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<18.0.0a0,>=14.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (17.0.0)\n",
            "Requirement already satisfied: pylibcudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.10.1)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (13.9.3)\n",
            "Requirement already satisfied: rmm-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.10.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (4.12.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12) (3.0.11)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12) (0.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.3dev0,>=2.0->cudf-cu12) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import cudf\n",
        " print(cudf.Series([1, 2, 3]))"
      ],
      "metadata": {
        "id": "h1hoWYYQ48Ev",
        "outputId": "4f6a3263-cd36-47a5-a484-cfaaaf69a5d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    1\n",
            "1    2\n",
            "2    3\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1 Contrucción de pipeline**"
      ],
      "metadata": {
        "id": "buGAXmiFJQlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgOT7c7NlNHE",
        "outputId": "c77938ec-1b1f-45ff-f8e6-1e9f7d6ebde6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  6 00:33:19 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0              27W /  70W |    103MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cudf\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from joblib import Parallel, delayed\n",
        "from gatspy.periodic import LombScargle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GVAZRWrLM7qC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar los datos"
      ],
      "metadata": {
        "id": "BH-pLldbJvvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El primer paso es cargar los datos de fotometría de los archivos CSV que contienen las observaciones de curvas de luz de diferentes tipos de objetos astronómicos. Se combinan las observaciones de Cepheid, RRL y EB en un único dataframe y se agregan las etiquetas correspondientes para la clasificación:\n",
        "\n",
        "- 0 para Cepheid.\n",
        "- 1 para RRL.\n",
        "- 2 para EB."
      ],
      "metadata": {
        "id": "CL-cHiwEJYFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_gpu(cepheid_path, rrl_path, eb_path):\n",
        "    cepheid_phot = cudf.read_csv(cepheid_path)\n",
        "    rrl_phot = cudf.read_csv(rrl_path)\n",
        "    eb_phot = cudf.read_csv(eb_path)\n",
        "\n",
        "    # Etiquetas para clasificación\n",
        "    cepheid_phot['label'] = 0\n",
        "    rrl_phot['label'] = 1\n",
        "    eb_phot['label'] = 2\n",
        "\n",
        "    # Combinar los datos en un solo DataFrame de CuDF\n",
        "    combined_data = cudf.concat([cepheid_phot, rrl_phot, eb_phot], ignore_index=True)\n",
        "    return combined_data"
      ],
      "metadata": {
        "id": "Rd3xv4AbdJGp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtrar la banda fotométrica i"
      ],
      "metadata": {
        "id": "p8GWEpJRJzOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente paso del pipeline es filtrar las observaciones para trabajar únicamente con las que se realizaron en la banda i, que representa una parte específica del espectro electromagnético (cerca del infrarrojo). Cada observación tiene un valor de flujo (FLUXCAL) en una banda fotométrica (BAND)."
      ],
      "metadata": {
        "id": "i2mevKjqJllg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para filtrar por banda\n",
        "class FilterBand(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, band='i'):\n",
        "        self.band = band\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X[X['BAND'] == self.band]"
      ],
      "metadata": {
        "id": "rxp_NaIbJe39"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalización de los datos (FLUXCAL)"
      ],
      "metadata": {
        "id": "6tokBmA_J1nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las redes neuronales funcionan mejor cuando todas las variables de entrada tienen magnitudes similares. Si los valores de flujo (FLUXCAL) tiene diferentes escalas, la red neuronal podría tener dificultades para aprender los patrones de forma eficiente.\n",
        "\n",
        "**FLUXCAL**: El flujo calibrado puede tener variaciones significativas entre observaciones, por lo que su normalización asegura que el modelo trate a todas las variables de manera equitativa."
      ],
      "metadata": {
        "id": "ZK5gYRceJ9tF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizeData(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        # Calcular media y desviación estándar en CuPy\n",
        "        self.mean = cp.mean(cp.array(X['FLUXCAL'].values))\n",
        "        self.std = cp.std(cp.array(X['FLUXCAL'].values))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Normalizar FLUXCAL usando CuPy\n",
        "        X['FLUXCAL'] = (cp.array(X['FLUXCAL'].values) - self.mean) / self.std\n",
        "        return X"
      ],
      "metadata": {
        "id": "EhCSm4V1KNad"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de secuencias temporales"
      ],
      "metadata": {
        "id": "MlcdffoOKQfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí creamos secuencias de observaciones de longitud fija a partir de los datos normalizados."
      ],
      "metadata": {
        "id": "S4EU78A3KUJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CreateSequences(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, sequence_length=10, stride=1, allow_incomplete_sequences=False):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.stride = stride\n",
        "        self.allow_incomplete_sequences = allow_incomplete_sequences\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        sequences = []\n",
        "\n",
        "        # Agrupar los datos por SNID para cada objeto\n",
        "        for obj_id, group in X.groupby('SNID'):\n",
        "            data = group.sort_values('MJD')  # Ordenar por tiempo\n",
        "            num_observations = len(data)\n",
        "\n",
        "            # Crear secuencias con la longitud deseada y el paso (stride)\n",
        "            for i in range(0, num_observations, self.stride):\n",
        "                sequence = data.iloc[i:i + self.sequence_length]\n",
        "                if len(sequence) == self.sequence_length:\n",
        "                    sequences.append(sequence)\n",
        "                elif self.allow_incomplete_sequences and len(sequence) > 1:\n",
        "                    sequences.append(sequence)\n",
        "\n",
        "        return pd.concat(sequences).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "gDG6C5NeMh0F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lomb-Scargle Periodogram"
      ],
      "metadata": {
        "id": "CSTrKOr4lmD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Lomb-Scargle Periodogram es una técnica usada para detectar periocidad en datos que son irregularmente muestreados (es decir, cuando los datos no están igualmente espaciados en el tiempo, como en muchas observaciones astronómicas)."
      ],
      "metadata": {
        "id": "EZHcWUe5lmD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddBestPeriod(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_jobs=-1):  # n_jobs=-1 usa todos los núcleos disponibles\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        if isinstance(X, tuple):\n",
        "            X, y = X\n",
        "\n",
        "        best_periods = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(self.calculate_best_period)(X[i, :, 0], X[i, :, 1]) for i in range(len(X))\n",
        "        )\n",
        "\n",
        "        best_periods = np.array(best_periods).reshape(-1, 1)\n",
        "        X_with_periods = np.concatenate([X, best_periods], axis=1)\n",
        "        return X_with_periods\n",
        "\n",
        "    def calculate_best_period(self, time, flux):\n",
        "        model = LombScargle(fit_period=True)\n",
        "        model.optimizer.period_range = (0.2, 1.2)\n",
        "        model.fit(time, flux)\n",
        "        return model.best_period\n"
      ],
      "metadata": {
        "id": "zMs3UernlmEA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline para el procesamiento completo"
      ],
      "metadata": {
        "id": "E_j2_fa3MpCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pipeline():\n",
        "    pipeline = Pipeline([\n",
        "        ('filter_band', FilterBand(band='i')),          # Filtrar la banda i\n",
        "        ('normalize', NormalizeData()),                 # Normalizar solo FLUXCAL en GPU\n",
        "        ('create_sequences', CreateSequences(sequence_length=10, stride=1, allow_incomplete_sequences=True)),  # Crear secuencias\n",
        "        ('add_best_period', AddBestPeriod(n_jobs=-1))   # Calcular el mejor período en paralelo\n",
        "    ])\n",
        "    return pipeline"
      ],
      "metadata": {
        "id": "3rtRorD3MsGj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def run_pipeline(cepheid_path, rrl_path, eb_path):\n",
        "    # Cargar los datos\n",
        "    combined_data = load_data_gpu(cepheid_path, rrl_path, eb_path)\n",
        "\n",
        "    # Crear el pipeline\n",
        "    pipeline = create_pipeline()\n",
        "\n",
        "    # Ejecutar el pipeline\n",
        "    X, y = pipeline.fit_transform(combined_data)\n",
        "\n",
        "    # Dividir los datos en conjunto de entrenamiento y validación\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Mostrar los tamaños de los conjuntos de datos\n",
        "    print(f'Tamaño del conjunto de entrenamiento: {X_train.shape}')\n",
        "    print(f'Tamaño del conjunto de validación: {X_val.shape}')\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n"
      ],
      "metadata": {
        "id": "mAWxFKlANf81"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rutas a los archivos\n",
        "cepheid_path = '/content/Cepheid_PHOT.csv'\n",
        "rrl_path = '/content/RRL_PHOT.csv'\n",
        "eb_path = '/content/EB_PHOT.csv'"
      ],
      "metadata": {
        "id": "vt2TLFBrNj6G"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejecutar pipeline"
      ],
      "metadata": {
        "id": "HAoW8GsXNu9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = run_pipeline(cepheid_path, rrl_path, eb_path)"
      ],
      "metadata": {
        "id": "clGCdvAmNkxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "032d4264-5941-44f8-ee4e-68fdf66c058a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CuPy failed to load libnvrtc.so.11.2: OSError: libnvrtc.so.11.2: cannot open shared object file: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32mcupy_backends/cuda/_softlink.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda._softlink.SoftLink.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: libnvrtc.so.11.2: cannot open shared object file: No such file or directory",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-80f7ad756a0a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcepheid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrrl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-b9e881869cd6>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(cepheid_path, rrl_path, eb_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Ejecutar el pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Dividir los datos en conjunto de entrenamiento y validación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-58f0c3dd3b8f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# Calcular media y desviación estándar en CuPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FLUXCAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FLUXCAL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cupy/_creation/from_data.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(obj, dtype, copy, order, subok, ndmin, blocking)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._array_from_cupy_ndarray\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.astype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.astype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc._get_ufunc_kernel\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._get_ufunc_kernel\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._get_simple_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._get_simple_elementwise_kernel_from_code\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.compile_with_cache\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.assemble_cupy_compiler_options\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/libs/nvrtc.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.getVersion\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/libs/_cnvrtc.pxi\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.initialize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/libs/_cnvrtc.pxi\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc._initialize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/libs/_cnvrtc.pxi\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc._get_softlink\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy_backends/cuda/_softlink.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda._softlink.SoftLink.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CuPy failed to load libnvrtc.so.11.2: OSError: libnvrtc.so.11.2: cannot open shared object file: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Entrenar modelo**"
      ],
      "metadata": {
        "id": "mnYYPFPmPRwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "De la forma en la que se preprocesaron los datos, se propone trabajar con dos modelos:\n",
        "- RNN (red neuronal recurrente)\n",
        "- LSTM (Long Short-Term Memory)"
      ],
      "metadata": {
        "id": "I3U0Th9MYhrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.1 LSTM**"
      ],
      "metadata": {
        "id": "bkJFbNxLYvPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las LSTM son ideales para este tipo de problemas porque pueden capturar la naturaleza secuencial de los datos, que es esencial para analizar las curvas de luz."
      ],
      "metadata": {
        "id": "W3nMQt9mw3Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "iTGMSUJ2qQZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "uS0XbkgFOfkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la forma de entrada y el número de clases\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # Longitud de secuencia, número de características\n",
        "num_classes = 3  # Cepheid, RRL, EB"
      ],
      "metadata": {
        "id": "vpKttcKVqjw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo y entrenarlo en GPU\n",
        "with tf.device('/device:GPU:0'):  # Asegura que se ejecute en la GPU si está disponible\n",
        "    model = build_lstm_model(input_shape, num_classes)"
      ],
      "metadata": {
        "id": "X_9nfVg8qmYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo en la GPU\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)"
      ],
      "metadata": {
        "id": "UXEOHhoxqpCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo en el conjunto de validación\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Pérdida en validación: {loss}')\n",
        "print(f'Precisión en validación: {accuracy}')"
      ],
      "metadata": {
        "id": "ySBqf1FdqrF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construir el modelo LSTM"
      ],
      "metadata": {
        "id": "JpW70-sbyXDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=input_shape))  # Capa LSTM\n",
        "    model.add(Dense(num_classes, activation='softmax'))  # Capa de salida\n",
        "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "bSlTLmH0x-y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir la forma de entrada y el número de clases"
      ],
      "metadata": {
        "id": "fTQ4vSN9yaw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (X_train.shape[1], X_train.shape[2])  # Longitud de secuencia, número de características\n",
        "num_classes = 3  # Cepheid, RRL, EB"
      ],
      "metadata": {
        "id": "V1LtqbVhyBfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear el modelo\n"
      ],
      "metadata": {
        "id": "dsoscEILygSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_lstm_model(input_shape, num_classes)"
      ],
      "metadata": {
        "id": "Ynjsu6YZyj1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenar el modelo\n",
        "\n"
      ],
      "metadata": {
        "id": "Xkura4ntylt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)"
      ],
      "metadata": {
        "id": "hvceVxRLyuS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluar el modelo en el conjunto de validación"
      ],
      "metadata": {
        "id": "P7Oc-JbuyoWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Pérdida en validación: {loss}')\n",
        "print(f'Precisión en validación: {accuracy}'"
      ],
      "metadata": {
        "id": "KGpdL_zAyDlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gráfica de la función de pérdida"
      ],
      "metadata": {
        "id": "Ja11zCiwyHaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
        "plt.title('Evolución de la pérdida durante el entrenamiento')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BOBS33ipyFyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicciones"
      ],
      "metadata": {
        "id": "zPDQJPgeyPok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar predicciones en el conjunto de validación\n",
        "predicciones = model.predict(X_val)\n",
        "predicciones_clases = np.argmax(predicciones, axis=1)\n",
        "\n",
        "# Mostrar las primeras 10 predicciones\n",
        "print(predicciones_clases[:10])"
      ],
      "metadata": {
        "id": "jBo6CN_7yRv6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}