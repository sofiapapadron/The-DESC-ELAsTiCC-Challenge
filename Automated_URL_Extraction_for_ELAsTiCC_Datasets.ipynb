{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK6y3dIkCnsOhqRJ/fgo2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiapapadron/The-DESC-ELAsTiCC-Challenge/blob/main/Automated_URL_Extraction_for_ELAsTiCC_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xdNqqVv_LtXK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP2uR5Ix1GDt"
      },
      "source": [
        "#**The DESC ELAsTiCC Challenge**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt6Nfwj8UXFd"
      },
      "source": [
        "# **0. Entendiemiento del negocio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3vsOrDyUahu"
      },
      "source": [
        "About ELAsTiCC\n",
        "\n",
        "The purpose of ELAsTiCC (\"Extended LSST Astronomical Time-series Classification Challenge\") is to spur the creation and testing of an end-to-end real-time pipeline for time-domain science. The challenge starts with a simulation of ~5 million detected events that includes ~50 million alerts. These alerts will be streamed from LSST to brokers, who will classify the events and send new alerts with classifications back to DESC. A talk about ELAsTiCC given at the LSSTC Enabling Science Broker Workshop in 2021 can be found on YouTube. Two posters on ELAsTiCC given at conferences can be found below on this page.\n",
        "\n",
        "For discussion or questions about the challenge, use the #elasticc-comms channel on the DESC Slack.\n",
        "\n",
        "The first ELAsTiCC campaign ran from September 2022 until early January 2023. Metrics and diagnostics from that campaign can be found on the ELAsTiCC page of the DESC TOM (login required).\n",
        "\n",
        "The Second ELAsTiCC campaign (dubbed ELAsTiCC2) ran from mid-November to mid-December 2023, streaing alerts at ~3× the rate of the first campaign. Diagnostics and some metrics from that campaign can be found on the ELAsTiCC2 page of the DESC TOM (login required).\n",
        "\n",
        "There is a new github repository for ELAsTiCC-related code and information: LSSTDESC/elasticc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**--> En este notebook trabajaremos la carga de los datasets desde la generación de un archivo .txt**"
      ],
      "metadata": {
        "id": "vBCzfpfTNU9f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTtrbgZCCdsz"
      },
      "source": [
        "#**1. Importar librerias**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpyP-QVKN_Zo",
        "outputId": "70d9d711-096a-4c27-fbd6-0acc38517529"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ],
      "metadata": {
        "id": "lZ2CnpdPOBdX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBWSbHxTC1qL"
      },
      "source": [
        "# **2. Carga de datos**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la página principal\n",
        "base_url = \"https://portal.nersc.gov/cfs/lsst/DESC_TD_PUBLIC/ELASTICC/TRAINING_SAMPLES/\"\n",
        "\n",
        "# Realizar una solicitud GET para obtener el contenido de la página\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Crear una lista para almacenar las URLs de los enlaces\n",
        "links = []\n",
        "\n",
        "# Iterar sobre todos los enlaces de la página\n",
        "for link in soup.find_all('a'):\n",
        "    href = link.get('href')\n",
        "    # Filtrar solo los enlaces relevantes\n",
        "    if href and not href.startswith('Parent Directory'):\n",
        "        # Construir la URL completa si es un enlace relativo\n",
        "        full_url = base_url + href if not href.startswith('http') else href\n",
        "        links.append(full_url)\n",
        "\n",
        "# Nombre del archivo donde se guardarán las URLs\n",
        "output_filename = 'all_links.txt'\n",
        "\n",
        "# Guardar las URLs en el archivo .txt\n",
        "with open(output_filename, 'w') as file:\n",
        "    for url in links:\n",
        "        file.write(url + '\\n')\n",
        "\n",
        "print(f\"Archivo '{output_filename}' creado con éxito con todas las URLs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Gis8GxNyPG",
        "outputId": "988ed386-d76e-4a41-f813-72ff7414a88a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo 'all_links.txt' creado con éxito con todas las URLs.\n"
          ]
        }
      ]
    }
  ]
}